# =============================================================================
# Recallium Configuration
# =============================================================================
# This file contains all configuration options for Recallium.
# Edit these values before running: docker compose --env-file recallium.env up -d
# =============================================================================

# -----------------------------------------------------------------------------
# PORT CONFIGURATION
# -----------------------------------------------------------------------------
# Map container ports to your host machine. Change these if you have conflicts.
# The container uses fixed internal ports; these control external access.
#
# Web UI: http://localhost:9001
HOST_UI_PORT=9001

# MCP API: http://localhost:8001/mcp
HOST_MCP_PORT=8001

# PostgreSQL: localhost:5433 (for external tools)
HOST_POSTGRES_PORT=5433

# IMPORTANT: If you change HOST_MCP_PORT, update your IDE config to match!

# -----------------------------------------------------------------------------
# DATA STORAGE
# -----------------------------------------------------------------------------
# Docker volume name for persistent data. Change if running multiple instances.
VOLUME_NAME=recallium-v1

# -----------------------------------------------------------------------------
# DATABASE
# -----------------------------------------------------------------------------
# PostgreSQL runs inside the container. Change password for production use.
POSTGRES_PASSWORD=recallium_password

# -----------------------------------------------------------------------------
# OLLAMA (Local LLM)
# -----------------------------------------------------------------------------
# If using Ollama for free local AI, configure the host URL.
# Default uses Docker's host.docker.internal to reach Ollama on your machine.
# For remote Ollama server, change to: http://your-ollama-server:11434
OLLAMA_HOST=http://host.docker.internal:11434

# -----------------------------------------------------------------------------
# DOCUMENT PROCESSING
# -----------------------------------------------------------------------------
# Configure how uploaded documents are chunked for search.
# Tokens per chunk (400 = safe with 22% margin)
# Increase for more context, decrease for precision
CHUNK_SIZE_TOKENS=400

# -----------------------------------------------------------------------------
# PERFORMANCE TUNING
# -----------------------------------------------------------------------------
# Adjust based on your hardware. Higher values = more resources, faster processing.

# Memories processed per batch
BATCH_SIZE=20

# Parallel operations limit
MAX_CONCURRENT=10

# Background worker threads
QUEUE_WORKERS=3

# -----------------------------------------------------------------------------
# FEATURE FLAGS
# -----------------------------------------------------------------------------
# Enable/disable specific features. Set to 'false' to disable.

# Auto-detect patterns across memories
ENABLE_PATTERN_MATCHING=true

# Generate AI insights from memory clusters
ENABLE_UNIFIED_INSIGHTS=true

# Process immediately (false = queue for batch)
REAL_TIME_PROCESSING=true

# -----------------------------------------------------------------------------
# NOTES
# -----------------------------------------------------------------------------
# - LLM provider configuration (API keys) is done via the Setup Wizard at
#   http://localhost:9001 after starting the container. Keys are stored in
#   an encrypted vault inside the container, not in this file.
#
# - Embeddings use the built-in GTE-Large model (free, runs locally).
#   No external API required for embeddings.
#
# - After editing this file, restart the container:
#   docker compose down && docker compose --env-file recallium.env up -d
# =============================================================================
