# =============================================================================
# Recallium Configuration
# =============================================================================
# This file contains all configuration options for Recallium.
# Edit these values before running: docker compose --env-file recallium.env up -d
# =============================================================================

# -----------------------------------------------------------------------------
# PORT CONFIGURATION
# -----------------------------------------------------------------------------
# Map container ports to your host machine. Change these if you have conflicts.
# The container uses fixed internal ports; these control external access.

HOST_UI_PORT=9001           # Web UI: http://localhost:9001
HOST_MCP_PORT=8001          # MCP API: http://localhost:8001/mcp
HOST_POSTGRES_PORT=5433     # PostgreSQL: localhost:5433 (for external tools)

# IMPORTANT: If you change HOST_MCP_PORT, update your IDE config to match!

# -----------------------------------------------------------------------------
# DATA STORAGE
# -----------------------------------------------------------------------------
# Docker volume name for persistent data. Change if running multiple instances.

VOLUME_NAME=recallium-v1

# -----------------------------------------------------------------------------
# DATABASE
# -----------------------------------------------------------------------------
# PostgreSQL runs inside the container. Change password for production use.

POSTGRES_PASSWORD=recallium_password    # Change this in production!

# -----------------------------------------------------------------------------
# OLLAMA (Local LLM)
# -----------------------------------------------------------------------------
# If using Ollama for free local AI, configure the host URL.
# Default uses Docker's host.docker.internal to reach Ollama on your machine.

OLLAMA_HOST=http://host.docker.internal:11434

# For remote Ollama server:
# OLLAMA_HOST=http://your-ollama-server:11434

# -----------------------------------------------------------------------------
# DOCUMENT PROCESSING
# -----------------------------------------------------------------------------
# Configure how uploaded documents are chunked for search.

CHUNK_SIZE_TOKENS=400       # Tokens per chunk (400 = safe with 22% margin)
                            # Increase for more context, decrease for precision

# -----------------------------------------------------------------------------
# PERFORMANCE TUNING
# -----------------------------------------------------------------------------
# Adjust based on your hardware. Higher values = more resources, faster processing.

BATCH_SIZE=20               # Memories processed per batch
MAX_CONCURRENT=10           # Parallel operations limit
QUEUE_WORKERS=3             # Background worker threads

# -----------------------------------------------------------------------------
# FEATURE FLAGS
# -----------------------------------------------------------------------------
# Enable/disable specific features. Set to 'false' to disable.

ENABLE_PATTERN_MATCHING=true    # Auto-detect patterns across memories
ENABLE_UNIFIED_INSIGHTS=true    # Generate AI insights from memory clusters
REAL_TIME_PROCESSING=true       # Process immediately (false = queue for batch)

# -----------------------------------------------------------------------------
# NOTES
# -----------------------------------------------------------------------------
# - LLM provider configuration (API keys) is done via the Setup Wizard at
#   http://localhost:9001 after starting the container. Keys are stored in
#   an encrypted vault inside the container, not in this file.
#
# - Embeddings use the built-in GTE-Large model (free, runs locally).
#   No external API required for embeddings.
#
# - After editing this file, restart the container:
#   docker compose down && docker compose --env-file recallium.env up -d
# =============================================================================
